{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acciones en Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Una vez que los datos se encuentran en el formato que necesitaba para llevar acabo el análisis de palabras, \n",
    "llegamos al punto de obtener los resultados. Para esto se introduce el segundo concepto de este artículo,las acciones. \n",
    "Al igual que con las transformaciones existen varios tipos acciones que nos pueden ayudar a mostrar los datos de diferentes maneras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Las siguientes son las acciones más comunes:**\n",
    "\n",
    "    -reduce\n",
    "    -collect\n",
    "    -count\n",
    "    -first\n",
    "    -take\n",
    "    -foreach\n",
    "    -countByKey\n",
    "    -takeSample\n",
    "    -takeOrdened\n",
    "    -saveAsTextFile\n",
    "    -saveAsSequenceFile\n",
    "    -saveAsObjectFile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce(func): agrega los elementos del dataset usando una función. Esta función debe ser conmutativa y asociativa para que pueda calcularse\n",
    "\n",
    "rdd_reduce = sc.parallelize([2,4,6,8])\n",
    "rdd_reduce.reduce(lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_multi = sc.parallelize([1,2,3,4])\n",
    "rdd_multi.reduce(lambda x,y: x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count(): devuelve el número de elementos del dataset.\n",
    "rdd = sc.parallelize(['j','t','h','d'])\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([item for item in range(10)])\n",
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola', 'spark', 'jeje']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collect(): convierte un RDD en un array y lo muestra en pantalla.\n",
    "rdd_context = sc.parallelize('Hola spark jeje'.split(' '))\n",
    "rdd_context.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La', 'programacion']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take(n): devuelve un array con los primeros elementos del dataset. Acá vemos la opción de parallelize en spark.\n",
    "rdd_take = sc.parallelize('La programacion es bella como camella'.split())\n",
    "rdd_take.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['La', 'programacion', 'es', 'bella']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_take.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#funcion max.- nos devuelve el maximo del el numero que este dentro del rdd\n",
    "rdd_max = sc.parallelize([item/(item+1)] for item in range (10))\n",
    "rdd_max.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0],\n",
       " [0.5],\n",
       " [0.6666666666666666],\n",
       " [0.75],\n",
       " [0.8],\n",
       " [0.8333333333333334],\n",
       " [0.8571428571428571],\n",
       " [0.875],\n",
       " [0.8888888888888888],\n",
       " [0.9]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_max.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d0711819d275f50fdf21fccbe0c6db757647d13ff622ffbd8a20209bb10ddb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
